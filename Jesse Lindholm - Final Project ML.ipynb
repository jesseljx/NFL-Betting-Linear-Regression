{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Final Project - NFL Betting with Multiple Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Discover which features in the NFL data sets tend to have the most correlation with the over under line and the spread favorite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I imported the necessary libraries/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2047,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I read both datasets into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2074,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['schedule_date', 'schedule_season', 'schedule_week', 'schedule_playoff',\n",
       "        'team_home', 'score_home', 'score_away', 'team_away',\n",
       "        'team_favorite_id', 'spread_favorite', 'over_under_line', 'stadium',\n",
       "        'stadium_neutral', 'weather_temperature', 'weather_wind_mph',\n",
       "        'weather_humidity', 'weather_detail'],\n",
       "       dtype='object'),\n",
       " Index(['Week', 'Day', 'Date', 'Time', 'Winner_tie', 'home_away', 'Loser_tie',\n",
       "        'Unnamed: 7', 'WPts', 'LPts', 'YdsW', 'TOW', 'YdsL', 'TOL'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 2074,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = 'spreadspoke_scores_2004-05_to_2020-21.csv'\n",
    "data2 = 'nfl_games_2-NEW.csv'\n",
    "df1 = pd.read_csv(data1)\n",
    "df2 = pd.read_csv(data2)\n",
    "df1.columns = df1.columns.str.strip()\n",
    "df2.columns = df2.columns.str.strip()\n",
    "df1.columns, df2.columns\n",
    "df1 = df1[0:256] #just 2004 season reg-season for simplicity of understanding operations\n",
    "df2 = df2[0:256] #just 2004 season reg-season for simplicity of understanding operations\n",
    "df1.columns, df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating a key column, I checked if the columns that I was using to create the key had the same elements. (More on this action below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2075,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
       "        '13', '14', '15', '16', '17'], dtype=object),\n",
       " array(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
       "        '13', '14', '15', '16', '17'], dtype=object))"
      ]
     },
     "execution_count": 2075,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.schedule_week.unique(), df2.Week.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I corrected the corresponding values in the two data sets that would go into making the keys. For example, both week column's 'Wildcards' which are Wildcard and WildCard. This code is relevant and useful when the dataframe includes playoff weeks which were cut out to reduce complications sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2076,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(df1['schedule_week']):\n",
    "    if w == 'WildCard':\n",
    "        df1.loc[i,'schedule_week'] = 'Wildcard'\n",
    "    elif w == 'SuperBowl':\n",
    "        df1.loc[i,'schedule_week'] = 'Superbowl'\n",
    "\n",
    "for i, w in enumerate(df2['Week']):\n",
    "    if w == 'WildCard':\n",
    "        df2.loc[i,'Week'] = 'Wildcard'\n",
    "    elif w == 'ConfChamp':\n",
    "        df2.loc[i,'Week'] = 'Conference'\n",
    "    elif w == 'SuperBowl':\n",
    "        df2.loc[i,'Week'] = 'Superbowl'\n",
    "\n",
    "print(list(df2.Week.unique()) == list(df1.schedule_week.unique()))\n",
    "\n",
    "two = list(df2.Winner_tie.unique())\n",
    "one = list(df1.team_home.unique())\n",
    "print(two.sort() == one.sort())\n",
    "#we want both of the values below to be True\n",
    "#when True, it means that the unique set of values of both data sets are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I created new columns in the data frame. Below the '#' I added to the new features to fit with the theme of just home features, away features, and aggregate features, inside the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2077,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Home_N'] = 0\n",
    "df2['Away_N'] = 0\n",
    "df2['Yds_home'] = 0\n",
    "df2['Yds_away'] = 0\n",
    "df2['TO_home'] = 0\n",
    "df2['TO_away'] = 0\n",
    "\n",
    "above_stats = ['Yds_home', 'Yds_away', 'TO_home', 'TO_away']\n",
    "\n",
    "#\n",
    "for i, row in enumerate(df2.iterrows()):\n",
    "    if df2.loc[i,'home_away'] == '@' or df2.loc[i,'home_away'] == 'N':\n",
    "        df2.loc[i, 'Home_N'] = df2.loc[i, 'Loser_tie']\n",
    "        df2.loc[i, 'Away_N'] = df2.loc[i, 'Winner_tie']\n",
    "        df2.loc[i, 'Yds_home'] = df2.loc[i, 'YdsL']\n",
    "        df2.loc[i, 'Yds_away'] = df2.loc[i, 'YdsW']\n",
    "        df2.loc[i, 'TO_home'] = df2.loc[i, 'TOL']\n",
    "        df2.loc[i, 'TO_away'] = df2.loc[i, 'TOW']\n",
    "        \n",
    "    elif df2.loc[i,'home_away'] != '@': #the text inside the string could be anything (not just @) because everything has been covered above. The key joining doesn't work if the expression says == '' though, so that's why it's expressed as != 'x' where x is a random letter, symbol, etc.\n",
    "        df2.loc[i, 'Away_N'] = df2.loc[i, 'Loser_tie']\n",
    "        df2.loc[i, 'Home_N'] = df2.loc[i, 'Winner_tie']\n",
    "        df2.loc[i, 'Yds_away'] = df2.loc[i, 'YdsL']\n",
    "        df2.loc[i, 'Yds_home'] = df2.loc[i, 'YdsW']\n",
    "        df2.loc[i, 'TO_away'] = df2.loc[i, 'TOL']\n",
    "        df2.loc[i, 'TO_home'] = df2.loc[i, 'TOW']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I made a separate year column for each data set because neither data set has a column that corresponds to just the year that the game was played. I made sure not to label both new columns the exact same thing; I created these column names with the lowercase and titlecase theme of each df in mind so I could know which data sets they were related to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2078,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df1['schedule_date'].notnull(), 'year'] = df1['schedule_date'].str[-4:]\n",
    "df2.loc[df2['Date'].notnull(), 'Year'] = df2['Date'].str[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Key\n",
    "To create a key, I first created a column named, 'key'. Then I let each element of the key be a string of values and dashes that made each row's key unique. the form of the key I made is \"[year]-[week]-[home team].\" As an example, the key could look like either of these examples: \n",
    "\n",
    "2022-15-Baltimore Ravens\n",
    "\n",
    "2004-Wildcard-Indianapolis Colts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2079,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2079-1bc5f5173108>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['key'][i] = str(df1.loc[i,'year'])+'-'+str(df1.loc[i,'schedule_week'])+'-'+str(df1.loc[i,'team_home'])\n",
      "<ipython-input-2079-1bc5f5173108>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['key'][i] = str(df2.loc[i,'Year'])+'-'+str(df2.loc[i,'Week'])+'-'+str(df2.loc[i,'Home_N'])\n"
     ]
    }
   ],
   "source": [
    "df1['key'] = ''\n",
    "\n",
    "for i, row in df1.iterrows():\n",
    "    df1['key'][i] = str(df1.loc[i,'year'])+'-'+str(df1.loc[i,'schedule_week'])+'-'+str(df1.loc[i,'team_home'])\n",
    "\n",
    "df2['key'] = ''\n",
    "for i, row in df2.iterrows():\n",
    "    df2['key'][i] = str(df2.loc[i,'Year'])+'-'+str(df2.loc[i,'Week'])+'-'+str(df2.loc[i,'Home_N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then reseted the index of the data set (back to 0,1,2,...) to use dfloc[i,'x'] ahead, where x is the name of a column in the dataframe AKA a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2080,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.set_index('key').join(df2.set_index('key'))\n",
    "df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating Additional Statistical Features\n",
    "The docstring covers what's going on here, but the purpose of creating these new stat columns is to use them in the future to find their correlation to a certain variable and to use them in the linear regression model. I also attempted to not create too much noise (which basically means, too much non-useful data that will reduce the model's accuracy) by creating all these columns. My goal was to add columns of valueable statistics to the dataframe which would help in find greater correlations between the target variables, spread_favorite and over_under_line, and thereby, likely creating a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2081,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistor_STD(dateframe):\n",
    "    '''iterates through each row and creates a new stat column (points, yards, turnovers),\n",
    "    both FOR and AGAINST both the home and away team, season to date'''\n",
    "    ha_list = ['_home', '_away']\n",
    "    stats = [('score', 'P'), ('Yds', 'Y'), ('TO', 'TO')]\n",
    "    for_away = ['F', 'A']\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        for ha in ha_list:\n",
    "            for stat in stats:\n",
    "                home_sum = df[(df.team_home==df.loc[i, 'team'+ha])&(df.Week.astype('int') < int(df.loc[i,'Week']))&(df.schedule_season==df.loc[i, 'schedule_season'])][stat[0]+'_home'].sum()\n",
    "                away_sum = df[(df.team_away==df.loc[i, 'team'+ha])&(df.Week.astype('int') < int(df.loc[i,'Week']))&(df.schedule_season==df.loc[i, 'schedule_season'])][stat[0]+'_home'].sum()\n",
    "                df.loc[i,'S'+stat[1]+'F'+ha] = home_sum+away_sum\n",
    "                #points against home STD\n",
    "                home_sum = df[(df.team_home==df.loc[i, 'team'+ha])&(df.Week.astype('int') < int(df.loc[i,'Week']))&(df.schedule_season==df.loc[i, 'schedule_season'])][stat[0]+'_away'].sum()\n",
    "                away_sum = df[(df.team_away==df.loc[i, 'team'+ha])&(df.Week.astype('int') < int(df.loc[i,'Week']))&(df.schedule_season==df.loc[i, 'schedule_season'])][stat[0]+'_away'].sum()\n",
    "                df.loc[i,'S'+stat[1]+'A'+ha] = home_sum+away_sum\n",
    "                #point differential home STD\n",
    "                df.loc[i,'S'+stat[1]+'D'+ha] = df.loc[i,'S'+stat[1]+'F'+ha]-df.loc[i,'S'+stat[1]+'A'+ha]\n",
    "\n",
    "statistor_STD(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is another stat column making function that is slightly different from the first: it creates related but slightly different values than the first function to be specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2082,
   "metadata": {},
   "outputs": [],
   "source": [
    "#difference explained in docstring\n",
    "def statistor_5_recent(dateframe):\n",
    "    '''iterates through each row and creates a new stat column (points, yards, turnovers), \n",
    "    both FOR and AGAINST both the home and away team, from the five most recent weeks'''\n",
    "    ha_list = ['_home', '_away']\n",
    "    stats = [('score', 'P'), ('Yds', 'Y'), ('TO', 'TO')]\n",
    "    for_away = ['F', 'A']\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        for ha in ha_list:\n",
    "            for stat in stats:\n",
    "                if int(df.loc[i,'Week'])-5 >= 1:\n",
    "                    home_sum = df[(df.team_home==df.loc[i, 'team'+ha])&(df.Week.astype('int') < int(df.loc[i,'Week']))&(df.Week.astype('int') > int(df.loc[i,'Week'])-5)&(df.schedule_season==df.loc[i, 'schedule_season'])][stat[0]+'_home'].sum()\n",
    "                    away_sum = df[(df.team_away==df.loc[i, 'team'+ha])&(df.Week.astype('int') < int(df.loc[i,'Week']))&(df.Week.astype('int') > int(df.loc[i,'Week'])-5)&(df.schedule_season==df.loc[i, 'schedule_season'])][stat[0]+'_home'].sum()\n",
    "                    df.loc[i,'5G'+stat[1]+'F'+ha] = home_sum+away_sum\n",
    "                    #points against home STD\n",
    "                    home_sum = df[(df.team_home==df.loc[i, 'team'+ha])&(df.Week.astype('int') < int(df.loc[i,'Week']))&(df.Week.astype('int') > int(df.loc[i,'Week'])-5)&(df.schedule_season==df.loc[i, 'schedule_season'])][stat[0]+'_away'].sum()\n",
    "                    away_sum = df[(df.team_away==df.loc[i, 'team'+ha])&(df.Week.astype('int') < int(df.loc[i,'Week']))&(df.Week.astype('int') > int(df.loc[i,'Week'])-5)&(df.schedule_season==df.loc[i, 'schedule_season'])][stat[0]+'_away'].sum()\n",
    "                    df.loc[i,'5G'+stat[1]+'A'+ha] = home_sum+away_sum\n",
    "                    #point differential home STD\n",
    "                    df.loc[i,'5G'+stat[1]+'D'+ha] = df.loc[i,'5G'+stat[1]+'F'+ha]-df.loc[i,'5G'+stat[1]+'A'+ha]\n",
    "\n",
    "statistor_5_recent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I dropped all of the irrelavent columns that didn't follow the main framework of home, away, aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2083,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['year', 'Week', 'Date', 'Winner_tie', 'home_away', 'Loser_tie', 'Unnamed: 7', 'WPts',\n",
    "       'LPts', 'YdsW', 'TOW', 'YdsL', 'TOL']\n",
    "\n",
    "for c in cols:\n",
    "    df = df.drop(columns=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot matrixing categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below first counts the frequency of the values, selects all of them, then extracts the name of index. I call this the mask. Then get_dummies is used on the filtered categorical values using the mask that was just created. Finally, the new dummy columns are merged with the data frame and the old categorical columns are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2084,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['schedule_date', 'schedule_week', 'schedule_playoff', 'team_home', 'team_away', 'team_favorite_id', 'stadium', 'stadium_neutral', 'weather_detail', 'Day', 'Time', 'Home_N', 'Away_N', 'Year']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    vals = df[column]\n",
    "    \n",
    "    mask = pd.value_counts(vals).nlargest(len(categorical_columns)).index\n",
    "    #IRRELEVANT INFO: mask = pd.value_counts(vals).nlargest(100).index #Counts the frequency of the values, selects the 100 largest by scope generosity, then extracts the name of index\n",
    "    #print(column,mask)\n",
    "\n",
    "    encoded_column = pd.get_dummies(pd.Categorical(vals, categories=mask), dtype=np.int64)\n",
    "\n",
    "    df = pd.merge(left=df,right=encoded_column,left_index=True,right_index=True,)\n",
    "\n",
    "    df = df.drop(columns=column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I filled any NaN values in the data frame with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2085,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2085-00ab233688f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#the NaN values in the 5G (5 most recent games) features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "#this gives an error, but continue running other cells after running this one, but at least it works in changing\n",
    "#the NaN values in the 5G (5 most recent games) features.\n",
    "for col in df.columns:\n",
    "    df[col]=df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below displays the R^2, the adjusted R^2 value, and the first 20 coefficients of the linear regression equation of a linear regression model with target variables, spread_favorite and over_under_line, predictor variables containing every feature from the data frame, except the target variables and any variables that would have been unknown before the game of each row. The correlation between the target variables and the rest of the values in the data frame are shown below each linear regression cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2061,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.5721744705585227\n",
      "Adjusted R^2: -0.38095582288071794\n",
      "[ 6.98544507e+10  4.62569825e-02  4.86838852e-02 -3.28479017e-02\n",
      "  2.60448116e+10 -2.60448116e+10 -2.60448116e+10 -4.19627246e+10\n",
      "  4.19627246e+10  4.19627246e+10  1.79630294e+10 -1.79630294e+10\n",
      " -1.79630294e+10  4.91994076e+10 -4.91994076e+10 -4.91994076e+10\n",
      " -4.28065430e+10  4.28065430e+10  4.28065430e+10  2.37440673e+08] ...\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "X = df.drop(['over_under_line', 'spread_favorite', 'score_home', 'score_away', 'Yds_home', 'Yds_away', 'TO_home', 'TO_away'], axis = 1)\n",
    "y = df['spread_favorite']\n",
    "model.fit(X, y)\n",
    "print(f\"R^2: {model.score(X,y)}\")\n",
    "print(f\"Adjusted R^2: {1 - (1-model.score(X, y))*(len(y)-1)/(len(y)-X.shape[1]-1)}\")\n",
    "print(model.coef_[0:20],'...')\n",
    "#df = df.drop(['over_under_line', 'score_home', 'score_away', 'WPts', 'LPts', 'YdsW', 'YdsL', 'TOW', 'TOL', 'Yds_home', 'Yds_away', 'TO_home', 'TO_away', ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2062,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spread_favorite          1.000000\n",
       "4:15PM                   0.145898\n",
       "Oakland Coliseum         0.144405\n",
       "5GPD_away                0.129908\n",
       "weather_temperature      0.126258\n",
       "11/14/2004               0.115749\n",
       "5GPD_home                0.114634\n",
       "Tennessee Titans_x       0.089580\n",
       "Tennessee Titans_x       0.089580\n",
       "8:35PM                   0.087561\n",
       "1                        0.086557\n",
       "10/31/2004               0.084715\n",
       "weather_wind_mph         0.082987\n",
       "2                        0.075297\n",
       "Soldier Field            0.073916\n",
       "EverBank Field           0.073916\n",
       "2004                     0.073890\n",
       "Yds_away                 0.072845\n",
       "score_away               0.071894\n",
       "8                        0.071266\n",
       "Baltimore Ravens_y       0.070000\n",
       "San Francisco 49ers_x    0.070000\n",
       "Baltimore Ravens_y       0.070000\n",
       "San Francisco 49ers_x    0.070000\n",
       "4                        0.068269\n",
       "Name: spread_favorite, dtype: float64"
      ]
     },
     "execution_count": 2062,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()['spread_favorite'].sort_values(ascending=False)[0:25]#['STOA_away']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2063,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7603120694676302\n",
      "Adjusted R^2: 0.22632376853475566\n",
      "[ 1.17853570e+11  5.96353844e-02  1.31538708e-02 -3.81317800e-02\n",
      "  4.81131197e+09 -4.81131197e+09 -4.81131197e+09 -7.75186100e+09\n",
      "  7.75186100e+09  7.75186100e+09  3.31834761e+09 -3.31834761e+09\n",
      " -3.31834761e+09  9.08870842e+09 -9.08870842e+09 -9.08870842e+09\n",
      " -7.90774132e+09  7.90774132e+09  7.90774132e+09  4.38629072e+07] ...\n"
     ]
    }
   ],
   "source": [
    "y = df['over_under_line']\n",
    "model.fit(X, y)\n",
    "print(f\"R^2: {model.score(X,y)}\")\n",
    "print(f\"Adjusted R^2: {1 - (1-model.score(X, y))*(len(y)-1)/(len(y)-X.shape[1]-1)}\")\n",
    "print(model.coef_[0:20],'...')\n",
    "#df = df.drop(['over_under_line', 'score_home', 'score_away', 'WPts', 'LPts', 'YdsW', 'YdsL', 'TOW', 'TOL', 'Yds_home', 'Yds_away', 'TO_home', 'TO_away', ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2064,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "over_under_line                 1.000000\n",
       "DOME                            0.459762\n",
       "Yds_away                        0.434906\n",
       "IND                             0.395454\n",
       "1:00PM                          0.371850\n",
       "Yds_home                        0.355279\n",
       "5GYA_home                       0.336483\n",
       "5GYF_away                       0.331813\n",
       "Indianapolis Colts_x            0.325816\n",
       "Indianapolis Colts_x            0.325816\n",
       "5GYA_away                       0.321733\n",
       "5GPF_home                       0.319426\n",
       "KC                              0.310738\n",
       "5GPA_away                       0.308827\n",
       "5GPA_home                       0.305350\n",
       "score_home                      0.293524\n",
       "score_away                      0.291394\n",
       "5GYF_home                       0.262275\n",
       "5GPF_away                       0.255978\n",
       "MIN                             0.251795\n",
       "SPA_home                        0.244134\n",
       "SPF_home                        0.228067\n",
       "Hubert H. Humphrey Metrodome    0.226629\n",
       "Minnesota Vikings_x             0.226629\n",
       "Minnesota Vikings_x             0.226629\n",
       "Name: over_under_line, dtype: float64"
      ]
     },
     "execution_count": 2064,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()['over_under_line'].sort_values(ascending=False)[0:25]#['STOA_away']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the four cells above tell me that clearly, the model is not very accurate in predicting the spread_favorite or the over_under_line for the 2004 season, but the over_underline has a better prediction. The purpose of integrating the extra stat values was to provide more correlation in the model.\n",
    "\n",
    "I slightly achieved this by having values like \n",
    "5GYA_home\n",
    "5GYF_away                       \n",
    "5GYA_away                       \n",
    "5GPF_home                       \n",
    "5GPA_away\n",
    "5GPA_home                      \n",
    "5GYF_home                       \n",
    "5GPF_away                                                    \n",
    "SPA_home                        \n",
    "SPF_home\n",
    "\n",
    "which all correlated from 0.22-0.33 for the over_under_line. Still these values are not ideal for a linear regression model, which is usually only useful when there are multiple values of high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below displays the R^2, the adjusted R^2 value, and the first 20 coefficients of the linear regression equation of a linear regression model with target variables, spread_favorite and over_under_line and predictor variables containing every feature from the data frame, except the target variables and any variables that would have been unknown before the game of each row. There is a slight difference in these linear regression models and the models above: each of the features drawn used in the model do not contain values from the first five weeks of the season. The purpose of this was to find how the '5G' columns would correlate with the target variables and influence the model's R^2 and adjust R^2 values. The correlation between the target variables and the rest of the values in the data frame are shown below each linear regression cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2091,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[75:] #restricts the data frame to data past first five weeks.\n",
    "#Run this instead of the four code cells above and then continue with running cells below for accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2087,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7366969383557912\n",
      "Adjusted R^2: -10.848637773989395\n",
      "[-3.15809516e-10  4.24788821e-02  5.30068485e-02 -1.04791659e-02\n",
      " -1.13951973e+11  1.13951973e+11  1.13951973e+11 -6.90095857e+10\n",
      "  6.90095857e+10  6.90095857e+10  1.11760384e+10 -1.11760384e+10\n",
      " -1.11760384e+10  2.45597864e+10 -2.45597864e+10 -2.45597864e+10\n",
      " -3.45428246e+10  3.45428246e+10  3.45428246e+10  3.47171282e+08] ...\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "X = df.drop(['over_under_line', 'spread_favorite', 'score_home', 'score_away', 'Yds_home', 'Yds_away', 'TO_home', 'TO_away'], axis = 1)\n",
    "y = df['spread_favorite']\n",
    "model.fit(X, y)\n",
    "print(f\"R^2: {model.score(X,y)}\")\n",
    "print(f\"Adjusted R^2: {1 - (1-model.score(X, y))*(len(y)-1)/(len(y)-X.shape[1]-1)}\")\n",
    "print(model.coef_[0:20],'...')\n",
    "#df = df.drop(['over_under_line', 'score_home', 'score_away', 'WPts', 'LPts', 'YdsW', 'YdsL', 'TOW', 'TOL', 'Yds_home', 'Yds_away', 'TO_home', 'TO_away', ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2088,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spread_favorite        1.000000\n",
       "5GPD_home              0.215888\n",
       "4:15PM                 0.179694\n",
       "Oakland Coliseum       0.166177\n",
       "11/14/2004             0.156600\n",
       "5GPD_away              0.155893\n",
       "KC                     0.122399\n",
       "10/31/2004             0.119870\n",
       "Soldier Field          0.107907\n",
       "8                      0.104832\n",
       "Tennessee Titans_x     0.102609\n",
       "Tennessee Titans_x     0.102609\n",
       "10/17/2004             0.101925\n",
       "8:35PM                 0.094226\n",
       "11/7/2004              0.094159\n",
       "Yds_away               0.093842\n",
       "6                      0.090486\n",
       "Arizona Cardinals_x    0.086718\n",
       "Arizona Cardinals_x    0.086718\n",
       "weather_temperature    0.086619\n",
       "SYD_home               0.086586\n",
       "Sun                    0.083520\n",
       "9                      0.079983\n",
       "EverBank Field         0.076123\n",
       "Dallas Cowboys_x       0.070826\n",
       "Name: spread_favorite, dtype: float64"
      ]
     },
     "execution_count": 2088,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()['spread_favorite'].sort_values(ascending=False)[0:25]#['STOA_away']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2089,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.9445773216755396\n",
      "Adjusted R^2: -1.4940205246007157\n",
      "[-2.46879307e-09  7.28007522e-02 -3.79072609e-02 -2.26271170e-02\n",
      " -5.35949570e+10  5.35949570e+10  5.35949570e+10 -3.24572335e+10\n",
      "  3.24572335e+10  3.24572335e+10  5.25641885e+09 -5.25641885e+09\n",
      " -5.25641885e+09  1.15511883e+10 -1.15511883e+10 -1.15511883e+10\n",
      " -1.62465042e+10  1.62465042e+10  1.62465042e+10  1.63284843e+08] ...\n"
     ]
    }
   ],
   "source": [
    "y = df['over_under_line']\n",
    "model.fit(X, y)\n",
    "print(f\"R^2: {model.score(X,y)}\")\n",
    "print(f\"Adjusted R^2: {1 - (1-model.score(X, y))*(len(y)-1)/(len(y)-X.shape[1]-1)}\")\n",
    "print(model.coef_[0:20],'...')\n",
    "#df = df.drop(['over_under_line', 'score_home', 'score_away', 'WPts', 'LPts', 'YdsW', 'YdsL', 'TOW', 'TOL', 'Yds_home', 'Yds_away', 'TO_home', 'TO_away', ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2090,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "over_under_line                 1.000000\n",
       "DOME                            0.468888\n",
       "Yds_away                        0.442880\n",
       "IND                             0.416175\n",
       "1:00PM                          0.378664\n",
       "5GYA_away                       0.372467\n",
       "5GYA_home                       0.365234\n",
       "5GYF_away                       0.358366\n",
       "5GPF_home                       0.354319\n",
       "Yds_home                        0.354006\n",
       "5GPA_away                       0.334817\n",
       "KC                              0.329203\n",
       "5GPA_home                       0.325814\n",
       "Indianapolis Colts_x            0.318662\n",
       "Indianapolis Colts_x            0.318662\n",
       "SPA_home                        0.308636\n",
       "5GPF_away                       0.307571\n",
       "SPF_home                        0.283094\n",
       "5GYF_home                       0.278855\n",
       "SPA_away                        0.276424\n",
       "score_away                      0.273835\n",
       "score_home                      0.262626\n",
       "MIN                             0.255169\n",
       "Minnesota Vikings_x             0.255065\n",
       "Hubert H. Humphrey Metrodome    0.255065\n",
       "Name: over_under_line, dtype: float64"
      ]
     },
     "execution_count": 2090,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()['over_under_line'].sort_values(ascending=False)[0:25]#['STOA_away']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the results from the four cells above tell me that clearly, the model is not very accurate in predicting the spread_favorite or the over_under_line for the 2004 season, but again, the over_underline has a better prediction. The purpose of integrating the extra stat values was to provide more correlation in the model.\n",
    "\n",
    "I slightly achieved this by having values like \n",
    "5GYA_home\n",
    "5GYF_away                       \n",
    "5GYA_away                       \n",
    "5GPF_home                       \n",
    "5GPA_away\n",
    "5GPA_home                      \n",
    "5GYF_home                       \n",
    "5GPF_away                                                    \n",
    "SPA_home                        \n",
    "SPF_home\n",
    "for the over_under_line which all correlated from 0.27-0.37. But this, again, is not ideal for a linear regression model.\n",
    "\n",
    "Of course the model performed worse in these cases with less data (five weeks worth less),\n",
    "but it makes sense that there was high correlation for the features starting with '5' because they did not\n",
    "have any data for the first five weeks of the season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "This project did not give me the results I wanted, but I still learned plenty of valuable information about using and manipulating data frames. In specific, I gained a lot of applicable knowledge about keys, the relationship between predictor variables and target variables in a linear regression model (and models in general), and finally creating new numerical variables within a data frame. I'm feel very accomplished about the work I did on this project and the problems I solved throughout the process, and I hope to revisit the project sometime, maybe if I get into the world of sports betting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
